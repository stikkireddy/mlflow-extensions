{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mlflow Extensions","text":"<p>The goal of this project is to make deploying any large language model, or multi modal large language models a simple three-step process.</p> <ol> <li>Download the model from hf or any other source.</li> <li>Register the model with mlflow.</li> <li>Deploy the model using the mlflow serving infrastructure. (e.g. Databricks)</li> </ol>"},{"location":"#framework-support-roadmap","title":"Framework Support / Roadmap","text":"<ul> <li> vLLM</li> <li> SGLang</li> <li> Ray Serving [WIP]</li> </ul> <p>This project will take those optimized model serving frameworks and deploy them to the following deployment targets.</p>"},{"location":"#deployment-clouds","title":"Deployment Clouds","text":"<ul> <li> AWS</li> <li> Azure</li> <li> GCP</li> </ul>"},{"location":"#deployment-targets","title":"Deployment Targets","text":"<ul> <li> Databricks Model Serving</li> <li> Databricks Job Cluster</li> <li> Databricks Interactive Clusters</li> </ul>"},{"location":"#deployment-modes","title":"Deployment Modes","text":"<ul> <li> EzDeployLite will ship a prebuilt configuration to databricks jobs. (dev/testing)</li> <li> EzDeploy will ship a prebuilt configuration to databricks model serving. (production)</li> </ul>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>mlflow-extensions is not developed, endorsed not supported by Databricks. It is provided as-is; no warranty is derived from using this package. For more details, please refer to the license.</p>"},{"location":"custom_server_architecture/","title":"EzDeploy Architecture","text":"<p>This document goes over how the custom server frameoworks like vLLM, sglang and ray serve work in the context mlflow-extensions. It tries to explain the architecture of the custom server frameworks and how they are integrated into the default scoring server for mlflow and databricks deployments.</p>"},{"location":"custom_server_architecture/#custom-supported-server-frameworks","title":"Custom Supported Server Frameworks","text":"<ol> <li>vLLM</li> <li>SGLang</li> <li>ray serve [WIP]</li> <li>ollama</li> </ol>"},{"location":"custom_server_architecture/#custom-pyfunc-implementation","title":"Custom Pyfunc Implementation","text":"<p>All of these frameworks typically run in their own processes and have their own command to boot up. Every framework is wrapped under a generic pyfunc to proxy /invocation requests to the custom server. The custom pyfunc also serializes and deserializes an httpx request and response object between the client (openai, httpx, sglang) to custom server implementation.</p> <p>Here is a high level diagram of the architecture:</p> <p></p> <p>Mlflow pyfuncs are split up into three main parts, <code>__init__</code>, <code>load_context</code> and <code>predict</code>. The constructor in this case is a very simple implementation. The <code>load_context</code> function is where the interesting bits happen.</p>"},{"location":"custom_server_architecture/#understanding-load_context","title":"Understanding <code>load_context</code>","text":"<p>Before we dive into <code>load_context</code> it is important to understand that when a model spawns in the scoring server it spawns multiple workers, in the case of mlflow it will use gunicorn as the server framework. Each worker will have its own instance of the model. This is important because some large models wont have enough resources if you are spawning more than one instance of the model. So instead <code>load_context</code> on the worker will call a \"filelock\" (using the filelock library) to lock various worker process using a filesystem file lock. This allows the various gunicorn processes to coordinate on a single leader which will spawn instances of the model serving framework (vLLM, sglang, etc). This is important because the model serving framework will have its own process and will be able to handle multiple requests at once. The last part of the <code>load_context</code> function is to run a separate process to ensure the server framework is healthy, and up and running. This is important, sometimes the SOTA server frameworks can be a bit unstable, throw segfaults, crash, etc. This health check restarts the server if it is down.</p> <p>Some room for improvements here would be to ensure that when the model is not in a healthy state that the requests should let the clients know that the model is not healthy. This is important because the serving endpoint may be up but the model may be down and in a recovering state.</p>"},{"location":"custom_server_architecture/#understanding-predict","title":"Understanding <code>predict</code>","text":"<p>The goal for predict is to proxy the request to the custom server. This is done by serializing the httpx Request into a json string and sending it to the custom server. The custom server will then deserialize the json string back into an httpx Request object and then run the request through the custom server. The custom server will then the proper response back to the scoring server. The scoring server will then serialize the response back into a json string and send it back to the client. The client will then deserialize the json string back into an httpx Response object. This is the biggest overhead in the system. The serialization and deserialization of the httpx Request and Response objects. This is important to note because the custom server will have to be able to handle the serialization and deserialization of the httpx efficiently. We have not currently measured the overhead of this.</p> <p>The trade off though mlflow scoring server does not support dynamic/adaptive batching. The client will have to handle batching requests to the scoring server. Most models like llms, transformers, etc will have a batching strategy to load batches of text, images, etc into the model and this framework will allow you to do that. So this is the trade off that you will have to make when using the custom server frameworks. (e2e latency over dynamic batching)</p>"},{"location":"custom_server_architecture/#compatability-clients","title":"Compatability Clients","text":"<p>The <code>mlflow_extensions.serving.compat</code> has a few clients that are compatible with the custom server frameworks. These clients are <code>OpenAI</code>, <code>ChatOpenAI</code>, <code>RuntimeEndpoint</code> (sglang), and standard httpx sync and async clients. These clients are used to serialize and deserialize requests to the custom server.</p>"},{"location":"custom_server_architecture/#request-flow","title":"Request flow","text":"<p>In this the pyfunc wrapper refers to the gunicorn workers/scoring server (mlflow specific workers)</p> <ol> <li>Compatability OpenAI Client sends a chat completion request</li> <li>Wrapper client intercepts the request and serializes the request into a json string</li> <li>Wrapper client sends the request to the pyfunc hosted in model serving</li> <li>Pyfunc wrapper deserializes the json string back into an httpx Request object</li> <li>Pyfunc wrapper runs the request through the custom server</li> <li>Custom server batches/executes the request</li> <li>Custom server sends the response back to the pyfunc wrapper</li> <li>Pyfunc wrapper serializes the response back into a json string</li> <li>Client deserializes the json string back into an httpx Response object</li> <li>Compatability OpenAI Client receives the response and transforms it into a chat completion response</li> </ol> <p>This is the detailed step by step process of how the request flows through the system. It all happens very quick but this is the indirection that is happening in the system.</p>"},{"location":"ezdeploy_lite_server_architecture/","title":"EzDeployLite Architecture [TBD]","text":""},{"location":"testing_new_configs/","title":"New Config Testing or Server Testing","text":"<p>This document is a guide to testing new configurations or servers. It is important to test new configurations or servers before deploying them to production. This document will guide you through the process of testing new configurations or servers.</p>"},{"location":"testing_new_configs/#integration-testing-new-configurations","title":"Integration Testing New Configurations","text":"<p>To test new configurations, follow these steps:</p> <ol> <li>Make sure you are using 15.4 LTS ML with an A100 GPU or 4xA10 GPUs for any model &lt; 70b fp16 parameters. Use 2xA100    for models &gt; 70b fp16 parameters.</li> <li>Construct the config and engines</li> <li>Test running the process and make sure you dont have health thread running</li> <li>Test the process with a simple model</li> <li>Stop the process or you will have to kill all processes running on the port</li> </ol> <p>Engine configurations can be found in:</p> <ol> <li>vllm: <code>mlflow_extensions/serving/engines/vllm_engine.py</code></li> <li>sglang: <code>mlflow_extensions/serving/engines/sglang_engine.py</code></li> </ol> <p>If you need flags from the server itself you can find them here:</p> <ol> <li>vllm: https://docs.vllm.ai/en/latest/models/engine_args.html</li> </ol> <p>You can use these additional args that are not built in to the <code>VLLMEngineConfig</code> model like so:</p> <pre><code>from mlflow_extensions.serving.engines import VLLMEngineConfig\n\nVLLMEngineConfig(\n    model=\"...\",\n    vllm_command_flags={\n        # args with actual values\n        \"--arg\": \"value\",\n        # flag that are truthy\n        \"--flag\": None\n    }\n)\n</code></pre>"},{"location":"testing_new_configs/#example-with-vllm-testing","title":"Example with vllm testing","text":"<p>Here is an example with vLLM but Sglang can be tested in a similar way.</p> <pre><code>from mlflow.pyfunc import PythonModelContext\nfrom mlflow_extensions.serving.engines import VLLMEngineProcess, VLLMEngineConfig\nfrom mlflow_extensions.testing.helper import kill_processes_containing\nfrom openai import OpenAI\n\n# kill any existing vllm processes\nkill_processes_containing(\"vllm.entrypoints.openai.api_server\")\nconfig = VLLMEngineConfig(\n    model=\"NousResearch/Hermes-3-Llama-3.1-8B\",\n    trust_remote_code=True,\n    guided_decoding_backend=\"outlines\",\n    max_model_len=64000,\n)\n\nartifacts = config.setup_artifacts()\n\nmlflow_ctx = PythonModelContext(\n    artifacts=artifacts,\n    model_config={}\n)\n\nnuextract_engine = VLLMEngineProcess(\n    config=config\n)\n\nnuextract_engine.start_proc(\n    context=mlflow_ctx,\n    health_check_thread=False  # make sure this is false it will keep spawning server if it shuts down\n)\n\nclient = OpenAI(\n    base_url=f\"http://{config.host}:{config.port}/v1\",\n    api_key=\"foo\")\n\nresponse = client.chat.completions.create(\n    model=config.model,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Why is the sky blue?\"}\n    ],\n    max_tokens=512\n)\nprint(response.choices[0].message.content)\n\n# shut down model using\nnuextract_engine.stop_proc()\n</code></pre>"},{"location":"testing_new_configs/#building-an-ezdeploy-config","title":"Building an EzDeploy Config","text":"<p>To build a EzDeploy Config you need to go to the following folders for the appropriate modality:</p> <ol> <li>audio: <code>mlflow_extensions/databricks/prebuilt/audio</code></li> <li>text: <code>mlflow_extensions/databricks/prebuilt/text</code></li> <li>vision: <code>mlflow_extensions/databricks/prebuilt/vision</code></li> </ol> <p>Please use the existing EzDeploy configs for reference and look at where the fields are being used.</p> <p>An EzDeploy Config needs to look like the following:</p> <pre><code>from mlflow_extensions.databricks.deploy.ez_deploy import (\n    EzDeployConfig,\n    ServingConfig,\n)\nfrom mlflow_extensions.serving.engines import VLLMEngineProcess, VLLMEngineConfig\n\n_ENGINE = VLLMEngineProcess\n_ENGINE_CONFIG = VLLMEngineConfig\n\nNEW_NOUS_CONFIG = EzDeployConfig(\n    # needs a name\n    name=\"hermes_3_llama_3_1_8b_64k\",\n    engine_proc=_ENGINE,\n    # the appropriate configs\n    engine_config=_ENGINE_CONFIG(\n        model=\"NousResearch/Hermes-3-Llama-3.1-8B\",\n        trust_remote_code=True,\n        guided_decoding_backend=\"outlines\",\n        max_model_len=64000,\n    ),\n    # the serving config, either estimated memory or specific gpus\n    serving_config=ServingConfig(\n        # rough estimate for the engines this includes model weights + kv cache + overhead + intermediate states\n        minimum_memory_in_gb=60,\n    ),\n)\n</code></pre> <p>In the previous code example we made: <code>NEW_NOUS_CONFIG</code> which is a <code>EzDeployConfig</code> object. We can add that to the registry by going to the bottom and looking for:</p> <pre><code>from dataclasses import dataclass\n\n\n@dataclass(frozen=True)\nclass VllmText:\n    ...\n</code></pre> <p>Then register the new model:</p> <pre><code>from dataclasses import dataclass, field\n\n\n@dataclass(frozen=True)\nclass VllmText:\n    NEW_NOUS_CONFIG = field(default_factory=lambda: NEW_NOUS_CONFIG)\n</code></pre> <p>Then go run the tests in the <code>mlflow_extensions/tests/integration/vllm</code> or <code>mlflow_extensions/tests/integration/sglang</code> folder to make sure the config is correct.</p>"},{"location":"examples/intro/","title":"Examples","text":""},{"location":"examples/intro/#instructions","title":"Instructions","text":"<ol> <li>Log in to your databricks workspace</li> <li>Run any cluster or serverless compute</li> <li>Clone the repo into the workspace or import specific notebooks</li> </ol>"},{"location":"examples/intro/#examples_1","title":"Examples","text":"<p>All examples can be found here: https://github.com/stikkireddy/mlflow-extensions/tree/main/examples/notebooks</p>"},{"location":"examples/ezdeploy/01-getting-started-phi-3.5-vision-instruct/","title":"Phi 3.5 Vision Instruct","text":"In\u00a0[0]: Copied! <pre>%pip install mlflow-extensions\n%pip install -U mlflow\ndbutils.library.restartPython()\n</pre> %pip install mlflow-extensions %pip install -U mlflow dbutils.library.restartPython() In\u00a0[0]: Copied! <pre>from mlflow_extensions.databricks.prebuilt import prebuilt\nfrom mlflow_extensions.databricks.deploy.ez_deploy import EzDeploy\n\ndeployer = EzDeploy(\n  config=prebuilt.vision.vllm.PHI_3_5_VISION_INSTRUCT_12K,\n  registered_model_name=\"main.default.sri_phi_3_5_vision_instruct_12k\"\n)\n</pre> from mlflow_extensions.databricks.prebuilt import prebuilt from mlflow_extensions.databricks.deploy.ez_deploy import EzDeploy  deployer = EzDeploy(   config=prebuilt.vision.vllm.PHI_3_5_VISION_INSTRUCT_12K,   registered_model_name=\"main.default.sri_phi_3_5_vision_instruct_12k\" ) In\u00a0[0]: Copied! <pre>deployer.download()\n</pre> deployer.download() In\u00a0[0]: Copied! <pre>deployer.register()\n</pre> deployer.register() In\u00a0[0]: Copied! <pre>endpoint_name = \"sri_phi_3_5_vision_instruct_vllm\"\ndeployer.deploy(endpoint_name)\n</pre> endpoint_name = \"sri_phi_3_5_vision_instruct_vllm\" deployer.deploy(endpoint_name) In\u00a0[0]: Copied! <pre>dbutils.notebook.exit()\n</pre> dbutils.notebook.exit() In\u00a0[0]: Copied! <pre>from mlflow_extensions.serving.compat.openai import OpenAI\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\n\nworkspace_host = spark.conf.get(\"spark.databricks.workspaceUrl\")\nendpoint_name = f\"https://{workspace_host}/serving-endpoints/{endpoint_name}/invocations\"\ntoken = get_databricks_host_creds().token\n</pre> from mlflow_extensions.serving.compat.openai import OpenAI from mlflow.utils.databricks_utils import get_databricks_host_creds  workspace_host = spark.conf.get(\"spark.databricks.workspaceUrl\") endpoint_name = f\"https://{workspace_host}/serving-endpoints/{endpoint_name}/invocations\" token = get_databricks_host_creds().token In\u00a0[0]: Copied! <pre>client = OpenAI(\n  base_url=endpoint_name,\n  api_key=token\n)\n\nmy_model = None\nfor model in client.models.list():\n  print(model.id)\n  my_model = model.id\n</pre> client = OpenAI(   base_url=endpoint_name,   api_key=token )  my_model = None for model in client.models.list():   print(model.id)   my_model = model.id In\u00a0[0]: Copied! <pre># Try this if you want to do guided decoding\n# from pydantic import BaseModel\n\n# class ExpectedJson(BaseModel):\n#     outside: bool\n#     inside: bool\n#     boardwalk: bool\n#     grass: bool\n\n# while True:\nresponse = client.chat.completions.create(\n  model=my_model,\n  messages=[\n    {\"role\": \"user\", \n    \"content\": [\n                {\"type\": \"text\", \"text\": \"Is the image indoors or outdoors?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                      \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n                    },\n                },\n            ],\n    }\n  ],\n  # extra_body={\n  #   \"guided_choice\": [\"outside\", \"indoors\"]\n  # }\n  # extra_body={\n  #   \"guided_json\": ExpectedJson.schema()\n  # }\n)\n\nresponse.choices[0].message.content.strip()\n</pre> # Try this if you want to do guided decoding # from pydantic import BaseModel  # class ExpectedJson(BaseModel): #     outside: bool #     inside: bool #     boardwalk: bool #     grass: bool  # while True: response = client.chat.completions.create(   model=my_model,   messages=[     {\"role\": \"user\",      \"content\": [                 {\"type\": \"text\", \"text\": \"Is the image indoors or outdoors?\"},                 {                     \"type\": \"image_url\",                     \"image_url\": {                       \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"                     },                 },             ],     }   ],   # extra_body={   #   \"guided_choice\": [\"outside\", \"indoors\"]   # }   # extra_body={   #   \"guided_json\": ExpectedJson.schema()   # } )  response.choices[0].message.content.strip() In\u00a0[0]: Copied! <pre>\n</pre>"},{"location":"examples/ezdeploy/01-getting-started-phi-3.5-vision-instruct/#wait-for-model-to-deploy","title":"Wait for model to deploy\u00b6","text":""},{"location":"examples/ezdeploy/02-getting-started-nuextract/","title":"NuExtract Text Model","text":"In\u00a0[0]: Copied! <pre>%pip install mlflow-extensions\n%pip install mlflow -U\ndbutils.library.restartPython()\n</pre> %pip install mlflow-extensions %pip install mlflow -U dbutils.library.restartPython() In\u00a0[0]: Copied! <pre>from mlflow_extensions.databricks.deploy.ez_deploy import EzDeploy\nfrom mlflow_extensions.databricks.prebuilt import prebuilt\n\ndeployer = EzDeploy(\n  config=prebuilt.text.vllm.NUEXTRACT,\n  registered_model_name=\"main.default.nuextract_vllm\"\n)\n\ndeployer.download()\n\ndeployer.register()\n\nendpoint_name = \"nuextract_vllm\"\n\ndeployer.deploy(endpoint_name)\n</pre> from mlflow_extensions.databricks.deploy.ez_deploy import EzDeploy from mlflow_extensions.databricks.prebuilt import prebuilt  deployer = EzDeploy(   config=prebuilt.text.vllm.NUEXTRACT,   registered_model_name=\"main.default.nuextract_vllm\" )  deployer.download()  deployer.register()  endpoint_name = \"nuextract_vllm\"  deployer.deploy(endpoint_name)  In\u00a0[0]: Copied! <pre>from mlflow_extensions.serving.compat.openai import OpenAI\nfrom mlflow_extensions.databricks.prebuilt import prebuilt\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\n\nworkspace_host = spark.conf.get(\"spark.databricks.workspaceUrl\")\nendpoint_name = \"nuextract_vllm\"\nendpoint_url = f\"https://{workspace_host}/serving-endpoints/{endpoint_name}/invocations\"\n\ntoken = get_databricks_host_creds().token\n\nclient = OpenAI(\n  base_url=endpoint_url,\n  api_key=token\n)\n\nmodel_name = prebuilt.text.vllm.NUEXTRACT.engine_config.model\n\nfrom pydantic import BaseModel\nfrom typing import Literal, List\n\nclass ExtractedBody(BaseModel):\n    product: str\n    languages: Literal[\"python\", \"sql\", \"scala\"]\n    keywords: List[str]\n    strategies: List[str]\n\n\nresponse = client.chat.completions.create(\n    model=model_name,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"The Databricks Lakehouse Platform for Dummies is your guide to simplifying \nyour data storage. The lakehouse platform has SQL and performance \ncapabilities - indexing, caching and MPP processing - to make \nBI work rapidly on data lakes. It also provides direct file access \nand direct native support for Python, data science and \nAI frameworks without the need to force data through an \nSQL-based data warehouse. Find out how the lakehouse platform \ncreates an opportunity for you to accelerate your data strategy.\"\"\"\n        }\n    ],\n    extra_body={\n        \"guided_json\": ExtractedBody.schema()\n    }\n)\nresponse.choices[0].message.content\n</pre> from mlflow_extensions.serving.compat.openai import OpenAI from mlflow_extensions.databricks.prebuilt import prebuilt from mlflow.utils.databricks_utils import get_databricks_host_creds  workspace_host = spark.conf.get(\"spark.databricks.workspaceUrl\") endpoint_name = \"nuextract_vllm\" endpoint_url = f\"https://{workspace_host}/serving-endpoints/{endpoint_name}/invocations\"  token = get_databricks_host_creds().token  client = OpenAI(   base_url=endpoint_url,   api_key=token )  model_name = prebuilt.text.vllm.NUEXTRACT.engine_config.model  from pydantic import BaseModel from typing import Literal, List  class ExtractedBody(BaseModel):     product: str     languages: Literal[\"python\", \"sql\", \"scala\"]     keywords: List[str]     strategies: List[str]   response = client.chat.completions.create(     model=model_name,     messages=[         {             \"role\": \"user\",             \"content\": \"\"\"The Databricks Lakehouse Platform for Dummies is your guide to simplifying  your data storage. The lakehouse platform has SQL and performance  capabilities - indexing, caching and MPP processing - to make  BI work rapidly on data lakes. It also provides direct file access  and direct native support for Python, data science and  AI frameworks without the need to force data through an  SQL-based data warehouse. Find out how the lakehouse platform  creates an opportunity for you to accelerate your data strategy.\"\"\"         }     ],     extra_body={         \"guided_json\": ExtractedBody.schema()     } ) response.choices[0].message.content In\u00a0[0]: Copied! <pre>\n</pre>"},{"location":"examples/ezdeploy/03-getting-started-llava-1.6-vicuna/","title":"LLaVa 1.6 Vicuna","text":"In\u00a0[0]: Copied! <pre>%pip install mlflow-extensions\n%pip install mlflow -U\n%pip install sglang==0.2.13 outlines==0.0.44\ndbutils.library.restartPython()\n</pre> %pip install mlflow-extensions %pip install mlflow -U %pip install sglang==0.2.13 outlines==0.0.44 dbutils.library.restartPython() In\u00a0[0]: Copied! <pre>from mlflow_extensions.databricks.deploy.ez_deploy import EzDeploy\nfrom mlflow_extensions.databricks.prebuilt import prebuilt\n\ndeployer = EzDeploy(\n  config=prebuilt.vision.sglang.LLAVA_NEXT_LLAMA3_8B_CONFIG,\n  registered_model_name=\"main.default.llava_next_llama3_8b_based\"\n)\n\ndeployer.download()\n\ndeployer.register()\n\nendpoint_name = \"llava_next_llama3_8b_sglang\"\n\ndeployer.deploy(endpoint_name)\n</pre> from mlflow_extensions.databricks.deploy.ez_deploy import EzDeploy from mlflow_extensions.databricks.prebuilt import prebuilt  deployer = EzDeploy(   config=prebuilt.vision.sglang.LLAVA_NEXT_LLAMA3_8B_CONFIG,   registered_model_name=\"main.default.llava_next_llama3_8b_based\" )  deployer.download()  deployer.register()  endpoint_name = \"llava_next_llama3_8b_sglang\"  deployer.deploy(endpoint_name) In\u00a0[0]: Copied! <pre>from mlflow_extensions.serving.compat.openai import OpenAI\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\n\nworkspace_host = spark.conf.get(\"spark.databricks.workspaceUrl\")\nendpoint_name = \"llava_next_llama3_8b_sglang\"\nendpoint_url = f\"https://{workspace_host}/serving-endpoints/{endpoint_name}/invocations\"\n\ntoken = get_databricks_host_creds().token\n\nclient = OpenAI(\n  base_url=endpoint_url,\n  api_key=token\n)\nclient = OpenAI(base_url=endpoint_url, api_key=token)\n# print(client.models.list())\nresponse = client.chat.completions.create(\n  model=\"lmms-lab/llama3-llava-next-8b\",\n  max_tokens=256,\n  messages=[\n    {\"role\": \"user\", \"content\": [\n                {\"type\": \"text\", \"text\": \"Explain the content of the image? What is in the background?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                      \"url\": \"https://richmedia.ca-richimage.com/ImageDelivery/imageService?profileId=12026540&amp;id=1859027&amp;recipeId=728\"\n                    },\n                },\n       ],\n     }\n  ],\n)\nprint(response.choices[0].message.content)\n</pre> from mlflow_extensions.serving.compat.openai import OpenAI from mlflow.utils.databricks_utils import get_databricks_host_creds  workspace_host = spark.conf.get(\"spark.databricks.workspaceUrl\") endpoint_name = \"llava_next_llama3_8b_sglang\" endpoint_url = f\"https://{workspace_host}/serving-endpoints/{endpoint_name}/invocations\"  token = get_databricks_host_creds().token  client = OpenAI(   base_url=endpoint_url,   api_key=token ) client = OpenAI(base_url=endpoint_url, api_key=token) # print(client.models.list()) response = client.chat.completions.create(   model=\"lmms-lab/llama3-llava-next-8b\",   max_tokens=256,   messages=[     {\"role\": \"user\", \"content\": [                 {\"type\": \"text\", \"text\": \"Explain the content of the image? What is in the background?\"},                 {                     \"type\": \"image_url\",                     \"image_url\": {                       \"url\": \"https://richmedia.ca-richimage.com/ImageDelivery/imageService?profileId=12026540&amp;id=1859027&amp;recipeId=728\"                     },                 },        ],      }   ], ) print(response.choices[0].message.content) In\u00a0[0]: Copied! <pre>from mlflow_extensions.serving.compat.sglang import RuntimeEndpoint\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\nfrom sglang import set_default_backend\nfrom sglang.srt.constrained import build_regex_from_object\nimport sglang as sgl\nfrom pydantic import BaseModel\nfrom typing import Literal\n\nimport requests\n\n# the first run takes a bit longer to compile the FSM on the server, all subsequent requests will be fast\n# the first call may take 10-30 seconds depending on the complexity of the pydantic object\n\ntoken = get_databricks_host_creds().token\n\n# connect sglang frontend (this python code) to the backend (model serving endpoint)\nset_default_backend(RuntimeEndpoint(endpoint_url, token))\n\n\nclass Fashion(BaseModel):\n    color: Literal[\"black\", \"blue\", \"gray\"]\n    material: Literal[\"silk\", \"denim\", \"fabric\"]\n    gender: Literal[\"male\", \"female\"]\n\nfashion = build_regex_from_object(Fashion)\n# fix a small regex bug with outlines + sglang for strings\nfashion = fashion.replace(r\"\"\"([^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\\\\\)\"\"\", \"[\\w\\d\\s]\")\nprint(fashion)\n\n@sgl.function\ndef image_qa(s, image_file):\n    s += sgl.user(sgl.image(image_file))\n    s += \"Fill in the details about the item... \\n\"\n    s += sgl.gen(\n        \"clothing_details\",\n        max_tokens=128,\n        temperature=0,\n        regex=fashion,  # Requires pydantic &gt;= 2.0\n    )\n\n\n# URL you want to fetch\nurl = \"https://richmedia.ca-richimage.com/ImageDelivery/imageService?profileId=12026540&amp;id=1859027&amp;recipeId=728\"\n\nresponse = requests.get(url)\nresponse.raise_for_status()  # Check for request errors\n\n# only need to send the bytes no download, etc\ndata = image_qa.run(\n  image_file=response.content,\n)\n\n# access by the generation key you asked for\nprint(data[\"clothing_details\"])\n</pre> from mlflow_extensions.serving.compat.sglang import RuntimeEndpoint from mlflow.utils.databricks_utils import get_databricks_host_creds from sglang import set_default_backend from sglang.srt.constrained import build_regex_from_object import sglang as sgl from pydantic import BaseModel from typing import Literal  import requests  # the first run takes a bit longer to compile the FSM on the server, all subsequent requests will be fast # the first call may take 10-30 seconds depending on the complexity of the pydantic object  token = get_databricks_host_creds().token  # connect sglang frontend (this python code) to the backend (model serving endpoint) set_default_backend(RuntimeEndpoint(endpoint_url, token))   class Fashion(BaseModel):     color: Literal[\"black\", \"blue\", \"gray\"]     material: Literal[\"silk\", \"denim\", \"fabric\"]     gender: Literal[\"male\", \"female\"]  fashion = build_regex_from_object(Fashion) # fix a small regex bug with outlines + sglang for strings fashion = fashion.replace(r\"\"\"([^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\\\\\)\"\"\", \"[\\w\\d\\s]\") print(fashion)  @sgl.function def image_qa(s, image_file):     s += sgl.user(sgl.image(image_file))     s += \"Fill in the details about the item... \\n\"     s += sgl.gen(         \"clothing_details\",         max_tokens=128,         temperature=0,         regex=fashion,  # Requires pydantic &gt;= 2.0     )   # URL you want to fetch url = \"https://richmedia.ca-richimage.com/ImageDelivery/imageService?profileId=12026540&amp;id=1859027&amp;recipeId=728\"  response = requests.get(url) response.raise_for_status()  # Check for request errors  # only need to send the bytes no download, etc data = image_qa.run(   image_file=response.content, )  # access by the generation key you asked for print(data[\"clothing_details\"])   In\u00a0[0]: Copied! <pre>from mlflow_extensions.serving.compat.sglang import RuntimeEndpoint\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\nfrom sglang import set_default_backend\nfrom sglang.srt.constrained import build_regex_from_object\nimport sglang as sgl\nfrom pydantic import BaseModel\nfrom typing import Literal\n\nimport requests\n\n# the first run takes a bit longer to compile the FSM on the server, all subsequent requests will be fast\n# the first call may take 10-30 seconds depending on the complexity of the pydantic object\n\ntoken = get_databricks_host_creds().token\n\n# connect sglang frontend (this python code) to the backend (model serving endpoint)\nset_default_backend(RuntimeEndpoint(endpoint_url, token))\n\n\nclass FashionProblems(BaseModel):\n    description: str\n    clothing_type: Literal[\"shirt\", \"pants\", \"dress\", \"skirt\", \"shoes\"]\n    color: Literal[\"black\", \"blue\", \"gray\"]\n    material: Literal[\"silk\", \"denim\", \"fabric\"]\n\nfashion_problems = build_regex_from_object(FashionProblems)\n# fix a small regex bug with outlines + sglang for strings\nfashion_problems = fashion_problems.replace(r\"\"\"([^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\\\\\)\"\"\", \"[\\w\\d\\s]\")\nprint(fashion_problems)\n\n@sgl.function\ndef image_qa(s, image_file):\n    s += sgl.user(sgl.image(image_file))\n    s += \"Fill in the problems about the product... \\n\"\n    s += sgl.gen(\n        \"clothing_details\",\n        max_tokens=128,\n        temperature=0,\n        regex=fashion_problems,  # Requires pydantic &gt;= 2.0\n    )\n\n\n# URL you want to fetch\nurl = \"https://m.media-amazon.com/images/I/51a94AxNRPL.jpg\"\n\nresponse = requests.get(url)\nresponse.raise_for_status()  # Check for request errors\n\n# only need to send the bytes no download, etc\ndata = image_qa.run(\n  image_file=response.content,\n)\n\n# access by the generation key you asked for\nprint(data[\"clothing_details\"])\n</pre> from mlflow_extensions.serving.compat.sglang import RuntimeEndpoint from mlflow.utils.databricks_utils import get_databricks_host_creds from sglang import set_default_backend from sglang.srt.constrained import build_regex_from_object import sglang as sgl from pydantic import BaseModel from typing import Literal  import requests  # the first run takes a bit longer to compile the FSM on the server, all subsequent requests will be fast # the first call may take 10-30 seconds depending on the complexity of the pydantic object  token = get_databricks_host_creds().token  # connect sglang frontend (this python code) to the backend (model serving endpoint) set_default_backend(RuntimeEndpoint(endpoint_url, token))   class FashionProblems(BaseModel):     description: str     clothing_type: Literal[\"shirt\", \"pants\", \"dress\", \"skirt\", \"shoes\"]     color: Literal[\"black\", \"blue\", \"gray\"]     material: Literal[\"silk\", \"denim\", \"fabric\"]  fashion_problems = build_regex_from_object(FashionProblems) # fix a small regex bug with outlines + sglang for strings fashion_problems = fashion_problems.replace(r\"\"\"([^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\\\\\)\"\"\", \"[\\w\\d\\s]\") print(fashion_problems)  @sgl.function def image_qa(s, image_file):     s += sgl.user(sgl.image(image_file))     s += \"Fill in the problems about the product... \\n\"     s += sgl.gen(         \"clothing_details\",         max_tokens=128,         temperature=0,         regex=fashion_problems,  # Requires pydantic &gt;= 2.0     )   # URL you want to fetch url = \"https://m.media-amazon.com/images/I/51a94AxNRPL.jpg\"  response = requests.get(url) response.raise_for_status()  # Check for request errors  # only need to send the bytes no download, etc data = image_qa.run(   image_file=response.content, )  # access by the generation key you asked for print(data[\"clothing_details\"])   In\u00a0[0]: Copied! <pre>from mlflow_extensions.serving.compat.sglang import RuntimeEndpoint\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\nfrom sglang import set_default_backend\nfrom sglang.srt.constrained import build_regex_from_object\nimport sglang as sgl\nfrom pydantic import BaseModel\nfrom typing import Literal, List\n\nimport requests\n\n# the first run takes a bit longer to compile the FSM on the server, all subsequent requests will be fast\n# the first call may take 10-30 seconds depending on the complexity of the pydantic object\n\ntoken = get_databricks_host_creds().token\n\n# connect sglang frontend (this python code) to the backend (model serving endpoint)\nset_default_backend(RuntimeEndpoint(endpoint_url, token))\n\n\nclass StockoutProblems(BaseModel):\n    description: str\n    stockout: bool\n    types_of_products: List[Literal[\"food\", \"clothing\", \"appliances\", \"durables\"]]\n\n\nstockout_problems = build_regex_from_object(StockoutProblems)\n# fix a small regex bug with outlines + sglang for strings\nstockout_problems = stockout_problems.replace(r\"\"\"([^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\\\\\)\"\"\", \"[\\w\\d\\s]\")\nprint(stockout_problems)\n\n@sgl.function\ndef image_qa(s, image_file):\n    s += sgl.user(sgl.image(image_file))\n    s += \"Is there an out of stock situation? What type of product seems to be out of stock? (food/clothing/appliances/durables) \\n\"\n    s += sgl.gen(\n        \"stockout_details\",\n        max_tokens=128,\n        temperature=0,\n        regex=stockout_problems,  # Requires pydantic &gt;= 2.0\n    )\n\n\n# URL you want to fetch\nurl = \"https://assets.eposnow.com/public/content-images/pexels-roy-broo-empty-shelves-grocery-items.jpg\"\n\nresponse = requests.get(url)\nresponse.raise_for_status()  # Check for request errors\n\n# only need to send the bytes no download, etc\ndata = image_qa.run(\n  image_file=response.content,\n)\n\n# access by the generation key you asked for\nprint(data[\"stockout_details\"])\n</pre> from mlflow_extensions.serving.compat.sglang import RuntimeEndpoint from mlflow.utils.databricks_utils import get_databricks_host_creds from sglang import set_default_backend from sglang.srt.constrained import build_regex_from_object import sglang as sgl from pydantic import BaseModel from typing import Literal, List  import requests  # the first run takes a bit longer to compile the FSM on the server, all subsequent requests will be fast # the first call may take 10-30 seconds depending on the complexity of the pydantic object  token = get_databricks_host_creds().token  # connect sglang frontend (this python code) to the backend (model serving endpoint) set_default_backend(RuntimeEndpoint(endpoint_url, token))   class StockoutProblems(BaseModel):     description: str     stockout: bool     types_of_products: List[Literal[\"food\", \"clothing\", \"appliances\", \"durables\"]]   stockout_problems = build_regex_from_object(StockoutProblems) # fix a small regex bug with outlines + sglang for strings stockout_problems = stockout_problems.replace(r\"\"\"([^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\\\\\)\"\"\", \"[\\w\\d\\s]\") print(stockout_problems)  @sgl.function def image_qa(s, image_file):     s += sgl.user(sgl.image(image_file))     s += \"Is there an out of stock situation? What type of product seems to be out of stock? (food/clothing/appliances/durables) \\n\"     s += sgl.gen(         \"stockout_details\",         max_tokens=128,         temperature=0,         regex=stockout_problems,  # Requires pydantic &gt;= 2.0     )   # URL you want to fetch url = \"https://assets.eposnow.com/public/content-images/pexels-roy-broo-empty-shelves-grocery-items.jpg\"  response = requests.get(url) response.raise_for_status()  # Check for request errors  # only need to send the bytes no download, etc data = image_qa.run(   image_file=response.content, )  # access by the generation key you asked for print(data[\"stockout_details\"])"},{"location":"examples/ezdeploy/04-getting-started-ultravox/","title":"Ultravox Audio Model","text":"In\u00a0[0]: Copied! <pre>%pip install mlflow-extensions\n%pip install mlflow -U\ndbutils.library.restartPython()\n</pre> %pip install mlflow-extensions %pip install mlflow -U dbutils.library.restartPython() In\u00a0[0]: Copied! <pre>from mlflow_extensions.databricks.deploy.ez_deploy import EzDeploy\nfrom mlflow_extensions.databricks.prebuilt import prebuilt\n\ndeployer = EzDeploy(\n    config=prebuilt.audio.vllm.FIXIE_ULTRA_VOX_0_4_64K_CONFIG,\n    registered_model_name=\"main.default.sri_ultravox_audio_text_to_text_model\"\n)\n\ndeployer.download()\n\ndeployer.register()\n\nendpoint_name = \"sri_ultravox_audio_text_to_text_model\"\n\ndeployer.deploy(endpoint_name)\n</pre> from mlflow_extensions.databricks.deploy.ez_deploy import EzDeploy from mlflow_extensions.databricks.prebuilt import prebuilt  deployer = EzDeploy(     config=prebuilt.audio.vllm.FIXIE_ULTRA_VOX_0_4_64K_CONFIG,     registered_model_name=\"main.default.sri_ultravox_audio_text_to_text_model\" )  deployer.download()  deployer.register()  endpoint_name = \"sri_ultravox_audio_text_to_text_model\"  deployer.deploy(endpoint_name) In\u00a0[0]: Copied! <pre>import requests\nimport base64\n\n\ndef encode_audio_base64_from_url(audio_url: str) -&gt; str:\n    \"\"\"Encode an audio retrieved from a remote url to base64 format.\"\"\"\n\n    with requests.get(audio_url) as response:\n        response.raise_for_status()\n        result = base64.b64encode(response.content).decode('utf-8')\n\n    return result\n\n\n# gettysburg.wav is a 17 second audio file\naudio_data = encode_audio_base64_from_url(\"https://www2.cs.uic.edu/~i101/SoundFiles/gettysburg.wav\")\n</pre> import requests import base64   def encode_audio_base64_from_url(audio_url: str) -&gt; str:     \"\"\"Encode an audio retrieved from a remote url to base64 format.\"\"\"      with requests.get(audio_url) as response:         response.raise_for_status()         result = base64.b64encode(response.content).decode('utf-8')      return result   # gettysburg.wav is a 17 second audio file audio_data = encode_audio_base64_from_url(\"https://www2.cs.uic.edu/~i101/SoundFiles/gettysburg.wav\") In\u00a0[0]: Copied! <pre>from mlflow_extensions.serving.compat.openai import OpenAI\nfrom mlflow_extensions.databricks.prebuilt import prebuilt\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\n\nworkspace_host = spark.conf.get(\"spark.databricks.workspaceUrl\")\nendpoint_name = \"sri_ultravox_audio_text_to_text_model\"\nendpoint_url = f\"https://{workspace_host}/serving-endpoints/{endpoint_name}/invocations\"\n\ntoken = get_databricks_host_creds().token\n\nclient = OpenAI(\n    base_url=endpoint_url,\n    api_key=token\n)\n\nmodel_name = prebuilt.audio.vllm.FIXIE_ULTRA_VOX_0_4_64K_CONFIG.engine_config.model\n\nchat_completion_from_base64 = client.chat.completions.create(\n    messages=[{\n        \"role\":\n            \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Breakdown the content of the audio?\"\n            },\n            {\n                \"type\": \"audio_url\",\n                \"audio_url\": {\n                    # Any format supported by librosa is supported\n                    \"url\": f\"data:audio/ogg;base64,{audio_data}\"\n                },\n            },\n        ],\n    }],\n    model=model_name,\n    max_tokens=512,\n)\n\nresult = chat_completion_from_base64.choices[0].message.content\nprint(result)\n</pre> from mlflow_extensions.serving.compat.openai import OpenAI from mlflow_extensions.databricks.prebuilt import prebuilt from mlflow.utils.databricks_utils import get_databricks_host_creds  workspace_host = spark.conf.get(\"spark.databricks.workspaceUrl\") endpoint_name = \"sri_ultravox_audio_text_to_text_model\" endpoint_url = f\"https://{workspace_host}/serving-endpoints/{endpoint_name}/invocations\"  token = get_databricks_host_creds().token  client = OpenAI(     base_url=endpoint_url,     api_key=token )  model_name = prebuilt.audio.vllm.FIXIE_ULTRA_VOX_0_4_64K_CONFIG.engine_config.model  chat_completion_from_base64 = client.chat.completions.create(     messages=[{         \"role\":             \"user\",         \"content\": [             {                 \"type\": \"text\",                 \"text\": \"Breakdown the content of the audio?\"             },             {                 \"type\": \"audio_url\",                 \"audio_url\": {                     # Any format supported by librosa is supported                     \"url\": f\"data:audio/ogg;base64,{audio_data}\"                 },             },         ],     }],     model=model_name,     max_tokens=512, )  result = chat_completion_from_base64.choices[0].message.content print(result) In\u00a0[0]: Copied! <pre>from mlflow_extensions.serving.compat.openai import OpenAI\nfrom mlflow_extensions.databricks.prebuilt import prebuilt\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\n\nworkspace_host = spark.conf.get(\"spark.databricks.workspaceUrl\")\nendpoint_name = \"sri_ultravox_audio_text_to_text_model\"\nendpoint_url = f\"https://{workspace_host}/serving-endpoints/{endpoint_name}/invocations\"\n\ntoken = get_databricks_host_creds().token\n\nclient = OpenAI(\n    base_url=endpoint_url,\n    api_key=token\n)\n\nmodel_name = prebuilt.audio.vllm.FIXIE_ULTRA_VOX_0_4_64K_CONFIG.engine_config.model\n\nfrom pydantic import BaseModel\nfrom typing import Literal, List\n\n\nclass AudioExtraction(BaseModel):\n    year: str\n    speaker: str\n    location: str\n    sentiment: Literal[\"positive\", \"negative\", \"neutral\"]\n    tone: Literal[\"somber\", \"upbeat\", \"pessemistic\"]\n    summary: str\n\n\nchat_completion_from_base64 = client.chat.completions.create(\n    messages=[{\n        \"role\":\n            \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": f\"Extract the following content from the audio: {str(AudioExtraction.schema())}?\"\n            },\n            {\n                \"type\": \"audio_url\",\n                \"audio_url\": {\n                    # Any format supported by librosa is supported\n                    \"url\": f\"data:audio/ogg;base64,{audio_data}\"\n                },\n            },\n        ],\n    }],\n    model=model_name,\n    max_tokens=512,\n    extra_body={\n        \"guided_json\": AudioExtraction.schema()\n    }\n)\n\nresult = chat_completion_from_base64.choices[0].message.content\nprint(result)\n</pre> from mlflow_extensions.serving.compat.openai import OpenAI from mlflow_extensions.databricks.prebuilt import prebuilt from mlflow.utils.databricks_utils import get_databricks_host_creds  workspace_host = spark.conf.get(\"spark.databricks.workspaceUrl\") endpoint_name = \"sri_ultravox_audio_text_to_text_model\" endpoint_url = f\"https://{workspace_host}/serving-endpoints/{endpoint_name}/invocations\"  token = get_databricks_host_creds().token  client = OpenAI(     base_url=endpoint_url,     api_key=token )  model_name = prebuilt.audio.vllm.FIXIE_ULTRA_VOX_0_4_64K_CONFIG.engine_config.model  from pydantic import BaseModel from typing import Literal, List   class AudioExtraction(BaseModel):     year: str     speaker: str     location: str     sentiment: Literal[\"positive\", \"negative\", \"neutral\"]     tone: Literal[\"somber\", \"upbeat\", \"pessemistic\"]     summary: str   chat_completion_from_base64 = client.chat.completions.create(     messages=[{         \"role\":             \"user\",         \"content\": [             {                 \"type\": \"text\",                 \"text\": f\"Extract the following content from the audio: {str(AudioExtraction.schema())}?\"             },             {                 \"type\": \"audio_url\",                 \"audio_url\": {                     # Any format supported by librosa is supported                     \"url\": f\"data:audio/ogg;base64,{audio_data}\"                 },             },         ],     }],     model=model_name,     max_tokens=512,     extra_body={         \"guided_json\": AudioExtraction.schema()     } )  result = chat_completion_from_base64.choices[0].message.content print(result) In\u00a0[0]: Copied! <pre>\n</pre>"},{"location":"examples/ezdeploy/05-getting-started-aya/","title":"Aya 35b Cohere Model","text":"In\u00a0[0]: Copied! <pre>%pip install mlflow-extensions\n%pip install -U mlflow\ndbutils.library.restartPython()\n</pre> %pip install mlflow-extensions %pip install -U mlflow dbutils.library.restartPython() In\u00a0[0]: Copied! <pre>from mlflow_extensions.databricks.deploy.ez_deploy import EzDeploy\nfrom mlflow_extensions.databricks.prebuilt import prebuilt\nimport os\n\nos.environ[\"HF_TOKEN\"] = dbutils.secrets.get(\n    scope=\"sri-mlflow-extensions\", key=\"hf-token\"\n)\n\ndeployer = EzDeploy(\n  config=prebuilt.text.vllm.COHERE_FOR_AYA_23_35B,\n  registered_model_name=\"main.default.cohere_aya_35b\"\n)\n\ndeployer.download()\n\ndeployer.register()\n\nendpoint_name = \"sri_cohere_aya\"\n\ndeployer.deploy(endpoint_name)\n</pre> from mlflow_extensions.databricks.deploy.ez_deploy import EzDeploy from mlflow_extensions.databricks.prebuilt import prebuilt import os  os.environ[\"HF_TOKEN\"] = dbutils.secrets.get(     scope=\"sri-mlflow-extensions\", key=\"hf-token\" )  deployer = EzDeploy(   config=prebuilt.text.vllm.COHERE_FOR_AYA_23_35B,   registered_model_name=\"main.default.cohere_aya_35b\" )  deployer.download()  deployer.register()  endpoint_name = \"sri_cohere_aya\"  deployer.deploy(endpoint_name) In\u00a0[0]: Copied! <pre>endpoint_name = \"sri_cohere_aya\"\n\nfrom mlflow_extensions.serving.compat.openai import OpenAI\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\nfrom mlflow_extensions.databricks.prebuilt import prebuilt\n\nworkspace_host = spark.conf.get(\"spark.databricks.workspaceUrl\")\nendpoint_name = f\"https://{workspace_host}/serving-endpoints/{endpoint_name}/invocations\"\ntoken = get_databricks_host_creds().token\n\nclient = OpenAI(\n  base_url=endpoint_name,\n  api_key=token\n)\n\nresponse = client.chat.completions.create(\n    model=prebuilt.text.vllm.COHERE_FOR_AYA_23_35B.engine_config.model,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Hi what model are you, who trained you?\"\n        }\n    ],\n)\nresponse\n</pre> endpoint_name = \"sri_cohere_aya\"  from mlflow_extensions.serving.compat.openai import OpenAI from mlflow.utils.databricks_utils import get_databricks_host_creds from mlflow_extensions.databricks.prebuilt import prebuilt  workspace_host = spark.conf.get(\"spark.databricks.workspaceUrl\") endpoint_name = f\"https://{workspace_host}/serving-endpoints/{endpoint_name}/invocations\" token = get_databricks_host_creds().token  client = OpenAI(   base_url=endpoint_name,   api_key=token )  response = client.chat.completions.create(     model=prebuilt.text.vllm.COHERE_FOR_AYA_23_35B.engine_config.model,     messages=[         {             \"role\": \"user\",             \"content\": \"Hi what model are you, who trained you?\"         }     ], ) response In\u00a0[0]: Copied! <pre>\n</pre>"},{"location":"examples/ezdeploy/05-getting-started-aya/#keep-in-mind-aya-does-not-have-a-apache-20-or-mit-license-it-is-cc-by-nc-40","title":"KEEP IN MIND AYA does not have a apache 2.0 or MIT license it is cc-by-nc-4.0\u00b6","text":""},{"location":"examples/ezdeploy/05-getting-started-aya/#you-must-adhere-to-c4ais-acceptable-use-policy","title":"YOU MUST adhere to C4AI's Acceptable Use Policy\u00b6","text":"<p>Read more here: https://huggingface.co/CohereForAI/aya-23-35B</p>"},{"location":"examples/ezdeploy/06-getting-started-qwen2-vl-instruct/","title":"Qwen2 VL Instruct","text":"In\u00a0[0]: Copied! <pre>%pip install mlflow-extensions==0.12.0\n%pip install -U mlflow\ndbutils.library.restartPython()\n</pre> %pip install mlflow-extensions==0.12.0 %pip install -U mlflow dbutils.library.restartPython() In\u00a0[0]: Copied! <pre>from mlflow_extensions.databricks.deploy.ez_deploy import EzDeploy\nfrom mlflow_extensions.databricks.prebuilt import prebuilt\nimport os\n\nos.environ[\"HF_TOKEN\"] = dbutils.secrets.get(\n    scope=\"sri-mlflow-extensions\", key=\"hf-token\"\n)\n\ndeployer = EzDeploy(\n  config=prebuilt.vision.vllm.QWEN2_VL_7B_INSTRUCT,\n  registered_model_name=\"main.default.qwen2_vl_7b_instruct\"\n)\n\ndeployer.download()\n\ndeployer.register()\n\nendpoint_name = \"sri_qwen2_vl_7b_instruct\"\n\ndeployer.deploy(endpoint_name, scale_to_zero=False)\n</pre> from mlflow_extensions.databricks.deploy.ez_deploy import EzDeploy from mlflow_extensions.databricks.prebuilt import prebuilt import os  os.environ[\"HF_TOKEN\"] = dbutils.secrets.get(     scope=\"sri-mlflow-extensions\", key=\"hf-token\" )  deployer = EzDeploy(   config=prebuilt.vision.vllm.QWEN2_VL_7B_INSTRUCT,   registered_model_name=\"main.default.qwen2_vl_7b_instruct\" )  deployer.download()  deployer.register()  endpoint_name = \"sri_qwen2_vl_7b_instruct\"  deployer.deploy(endpoint_name, scale_to_zero=False) In\u00a0[0]: Copied! <pre>endpoint_name = \"sri_qwen2_vl_7b_instruct\"\n\nfrom mlflow_extensions.serving.compat.openai import OpenAI\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\nfrom mlflow_extensions.databricks.prebuilt import prebuilt\nfrom pydantic import BaseModel\nimport typing as t\n\nworkspace_host = spark.conf.get(\"spark.databricks.workspaceUrl\")\nendpoint_name = f\"https://{workspace_host}/serving-endpoints/{endpoint_name}/invocations\"\ntoken = get_databricks_host_creds().token\n\nclient = OpenAI(\n  base_url=endpoint_name,\n  api_key=token\n)\n\n\nclass Comparison(BaseModel):\n  image_1_details: str\n  image_2_details: str\n  image_1_colors: t.List[str]\n  image_2_colors: t.List[str]\n  image_1_has_human: bool\n  image_2_has_human: bool\n  image_1_human_gender: t.Literal[\"male\", \"female\", \"no human\"]\n  image_2_human_gender: t.Literal[\"male\", \"female\", \"no human\"]\n\nprompt = f\"Compare the two images and use this schema for reference and no markdown just valid json: {Comparison.schema()}\"\n\n\n\nurl_1 = \"https://richmedia.ca-richimage.com/ImageDelivery/imageService?profileId=12026540&amp;id=1859027&amp;recipeId=728\"\nurl_2 = \"https://m.media-amazon.com/images/I/81W3YQdu-tL._AC_SY550_.jpg\"\n\nresponse = client.chat.completions.create(\n    model=prebuilt.vision.vllm.QWEN2_VL_7B_INSTRUCT.engine_config.model,\n    messages=[\n      {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\" : \"text\", \"text\": prompt},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": url_1}},\n            {\"type\" : \"text\", \"text\": \"to this image. Answer in english.\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": url_2}}\n        ]\n      }\n    ],\n    max_tokens=8192,\n)\nprint(response.choices[0].message.content)\n</pre> endpoint_name = \"sri_qwen2_vl_7b_instruct\"  from mlflow_extensions.serving.compat.openai import OpenAI from mlflow.utils.databricks_utils import get_databricks_host_creds from mlflow_extensions.databricks.prebuilt import prebuilt from pydantic import BaseModel import typing as t  workspace_host = spark.conf.get(\"spark.databricks.workspaceUrl\") endpoint_name = f\"https://{workspace_host}/serving-endpoints/{endpoint_name}/invocations\" token = get_databricks_host_creds().token  client = OpenAI(   base_url=endpoint_name,   api_key=token )   class Comparison(BaseModel):   image_1_details: str   image_2_details: str   image_1_colors: t.List[str]   image_2_colors: t.List[str]   image_1_has_human: bool   image_2_has_human: bool   image_1_human_gender: t.Literal[\"male\", \"female\", \"no human\"]   image_2_human_gender: t.Literal[\"male\", \"female\", \"no human\"]  prompt = f\"Compare the two images and use this schema for reference and no markdown just valid json: {Comparison.schema()}\"    url_1 = \"https://richmedia.ca-richimage.com/ImageDelivery/imageService?profileId=12026540&amp;id=1859027&amp;recipeId=728\" url_2 = \"https://m.media-amazon.com/images/I/81W3YQdu-tL._AC_SY550_.jpg\"  response = client.chat.completions.create(     model=prebuilt.vision.vllm.QWEN2_VL_7B_INSTRUCT.engine_config.model,     messages=[       {         \"role\": \"user\",         \"content\": [             {\"type\" : \"text\", \"text\": prompt},             {\"type\": \"image_url\", \"image_url\": {\"url\": url_1}},             {\"type\" : \"text\", \"text\": \"to this image. Answer in english.\"},             {\"type\": \"image_url\", \"image_url\": {\"url\": url_2}}         ]       }     ],     max_tokens=8192, ) print(response.choices[0].message.content) In\u00a0[0]: Copied! <pre>\n</pre>"},{"location":"examples/ezdeploylite/intro/","title":"EzDeployLite [TBD]","text":""},{"location":"getting-started/audio-models/","title":"Audio Language Models","text":"<p>Audio Language models are models that are trained on audio tasks. These models can be used for various audio tasks like summarization, transcription, etc.</p> <p>You can use <code>EzDeploy</code> or <code>EzDeployLite</code> to deploy these models to model serving. Read the previous guides. For the scope of this we will be using <code>EzDeployLite</code> to deploy a audio model.</p>"},{"location":"getting-started/audio-models/#ezdeploylite","title":"EzDeployLite","text":"<pre><code>%pip install mlflow-extensions==0.14.0\ndbutils.library.restartPython()\n\nfrom mlflow_extensions.databricks.deploy.ez_deploy import EzDeployLite\nfrom mlflow_extensions.databricks.prebuilt import prebuilt\n\ndeployer = EzDeployLite(\n    ez_deploy_config=prebuilt.audio.vllm.FIXIE_ULTRA_VOX_0_4_64K_CONFIG\n)\n\ndeployment_name = \"my_ultravox_model\"\ndeployer.deploy(deployment_name)\n</code></pre> <p>The code will deploy a audio model to Databricks jobs and expose the model as via a proxy. This is meant for dev and testing use cases.</p>"},{"location":"getting-started/audio-models/#querying-using-openai-sdk-for-audio-models","title":"Querying using OpenAI SDK for Audio Models","text":""},{"location":"getting-started/audio-models/#using-audio-file-urls","title":"Using Audio File URLs","text":"<p>You can query the model using image URLs. The model will return the desired output based on the image provided.</p> <p>Installation:</p> <pre><code>%pip install mlflow-extensions==0.14.0\n%pip install -U openai\ndbutils.library.restartPython()\n</code></pre> <p>Query Model:</p> <pre><code>from mlflow_extensions.serving.compat import get_ezdeploy_lite_openai_url\n\ndeployment_name = \"my_ultravox_model\"\nbase_url = get_ezdeploy_lite_openai_url(deployment_name)\n\nfrom openai import OpenAI\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\n\nclient = OpenAI(base_url=base_url, api_key=get_databricks_host_creds().token)\nfor i in client.models.list():\n    model = i.id\n\nprint(model)\n\nresponse = client.chat.completions.create(\n    model=\"default\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Is the image indoors or outdoors?\"},\n                {\n                    \"type\": \"audio_url\",\n                    \"audio_url\": {\n                        \"url\": \"https://www2.cs.uic.edu/~i101/SoundFiles/gettysburg.wav\"\n                    },\n                },\n            ],\n        }\n    ],\n)\nprint(response)\n</code></pre>"},{"location":"getting-started/audio-models/#using-audio-clips-from-base64-encoded-strings","title":"Using Audio clips from base64 encoded strings","text":"<p>You can also query the model using images from the file system. The model will return the desired output based on the image provided.</p> <p>Installation:</p> <pre><code>%pip install mlflow-extensions==0.14.0\n%pip install -U openai\ndbutils.library.restartPython()\n</code></pre> <p>Download Image into Base64; you can modify this to fetch from file system but the logic would be the same.</p> <p>Query Model:</p> <pre><code>import base64\nimport requests\nfrom mlflow_extensions.serving.compat import get_ezdeploy_lite_openai_url\n\ndeployment_name = \"my_qwen_model\"\nbase_url = get_ezdeploy_lite_openai_url(deployment_name)\n\nfrom openai import OpenAI\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\n\nclient = OpenAI(base_url=base_url, api_key=get_databricks_host_creds().token)\nfor i in client.models.list():\n    model = i.id\n\nprint(model)\n\n\n## Use base64 encoded image in the payload\ndef encode_audio_base64_from_url(audio_url: str) -&gt; str:\n    \"\"\"Encode an image retrieved from a remote url to base64 format.\"\"\"\n\n    # you can modify this with reading an image from a file\n    with requests.get(audio_url) as response:\n        response.raise_for_status()\n        result = base64.b64encode(response.content).decode('utf-8')\n\n    return result\n\n\n# gettysburg.wav is a 17 second audio file\naudio_data = encode_audio_base64_from_url(\"https://www2.cs.uic.edu/~i101/SoundFiles/gettysburg.wav\")\nchat_completion_from_base64 = client.chat.completions.create(\n    model=\"default\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"What\u2019s in this image?\"\n            },\n            {\n                \"type\": \"audio_url\",\n                \"audio_url\": {\n                    # Any format supported by librosa is supported\n                    \"url\": f\"data:audio/ogg;base64,{audio_data}\"\n                },\n            },\n        ],\n    }],\n    max_tokens=256,\n)\nprint(chat_completion_from_base64)\n</code></pre>"},{"location":"getting-started/custom-engines/","title":"Using Custom Engines","text":"<p>If you do not want to use any of the prebuilts this guide will help you deploy your the custom engine directly.</p>"},{"location":"getting-started/custom-engines/#deploying-models-using-vllm","title":"Deploying Models using vLLM","text":"<p>vLLM is a optimized server that is optimized for running llms and multimodal lms. It is a complex server that supports a lot of configuration/knobs to improve performance. This documentation will be updated as we test more configurations.</p>"},{"location":"getting-started/custom-engines/#registering-a-model","title":"Registering a model","text":"<pre><code>import mlflow\n\nfrom mlflow_extensions.serving.engines import VLLMEngineProcess, VLLMEngineConfig\nfrom mlflow_extensions.serving.wrapper import CustomServingEnginePyfuncWrapper\n\nmlflow.set_registry_uri(\"databricks-uc\")\n\n# optionally if you need to download model from hf which is not public facing\n# os.environ[\"HF_TOKEN\"] = ...\n\nmodel = CustomServingEnginePyfuncWrapper(\n    engine=VLLMEngineProcess,\n    engine_config=VLLMEngineConfig(\n        model=\"microsoft/Phi-3.5-vision-instruct\",\n        trust_remote_code=True,\n        max_model_len=64000,  # max token length for context\n        guided_decoding_backend=\"outlines\"\n    )\n)\n\nmodel.setup()  # download artifacts from huggingface\n\nwith mlflow.start_run() as run:\n    mlflow.pyfunc.log_model(\n        \"model\",\n        python_model=model,\n        artifacts=model.artifacts,\n        pip_requirements=model.get_pip_reqs(),\n        registered_model_name=\"&lt;catalog&gt;.&lt;schema&gt;.&lt;model-name&gt;\"\n    )\n</code></pre>"},{"location":"getting-started/custom-engines/#deploying-models-using-sglang-tbd","title":"Deploying Models using SGLang [TBD]","text":""},{"location":"getting-started/custom-engines/#deploying-models-using-ray-serving-tbd","title":"Deploying Models using Ray Serving [TBD]","text":""},{"location":"getting-started/custom-engines/#deploying-models-using-ollama","title":"Deploying Models using Ollama","text":"<p>Ollama is a optimized server that is optimized for running llms and multimodal lms. It supports llama.cpp as the backend to be able to run the models using cpu and ram. This documentation will be updated as we test more configurations.</p> <p>**Keep in mind databricks serving endpoints only have 4gb of memory per container. ** Link to docs.</p>"},{"location":"getting-started/custom-engines/#registering-a-model_1","title":"Registering a model","text":"<pre><code>import mlflow\n\nfrom mlflow_extensions.serving.engines import OllamaEngineConfig, OllamaEngineProcess\nfrom mlflow_extensions.serving.wrapper import CustomServingEnginePyfuncWrapper\n\nmlflow.set_registry_uri(\"databricks-uc\")\n\nmodel = CustomServingEnginePyfuncWrapper(\n    engine=OllamaEngineProcess,\n    engine_config=OllamaEngineConfig(\n        model=\"gemma2:2b\",\n    )\n)\n\nmodel.setup()  # this will download ollama and the model. it may take a while so let it run.\n\nwith mlflow.start_run() as run:\n    mlflow.pyfunc.log_model(\n        \"model\",\n        python_model=model,\n        artifacts=model.artifacts,\n        pip_requirements=model.get_pip_reqs(),\n        registered_model_name=\"&lt;catalog&gt;.&lt;schema&gt;.&lt;model-name&gt;\"\n    )\n</code></pre>"},{"location":"getting-started/ezdeploy/","title":"Deploying a model with EzDeploy","text":""},{"location":"getting-started/ezdeploy/#requirements","title":"Requirements","text":"<ol> <li>You need access to model serving in your region</li> <li>Your region needs to support gpus (T4, A10, A100, or H100)</li> <li>You need to have access to any compute to run the script from a notebook. (Serverless or interactive)</li> <li>Access to unity catalog schema to register the model.</li> </ol>"},{"location":"getting-started/ezdeploy/#what-is-ezdeploy","title":"What is EzDeploy?","text":"<p>EzDeploy will take a prebuilt configuration and deploy it to databricks model serving. This is meant for production use cases. It will support either vLLM or SGLang as engines.</p>"},{"location":"getting-started/ezdeploy/#deployment-steps","title":"Deployment Steps","text":""},{"location":"getting-started/ezdeploy/#1-install-the-library","title":"1. Install the library","text":"<pre><code>%pip install mlflow-extensions\ndbutils.library.restartPython()\n</code></pre>"},{"location":"getting-started/ezdeploy/#2-identify-the-model-to-deploy","title":"2. Identify the model to deploy","text":"<p>In this scenario we will deploy a Nous Hermes model to model serving.</p> <pre><code>from mlflow_extensions.databricks.deploy.ez_deploy import EzDeploy\nfrom mlflow_extensions.databricks.prebuilt import prebuilt\n\ndeployer = EzDeploy(\n    # The model config to deploy\n    config=prebuilt.text.vllm.NOUS_HERMES_3_LLAMA_3_1_8B_64K,\n    # The model to register in unity catalog\n    registered_model_name=\"main.default.nous_research_hermes_3_1\"\n)\n\ndeployer.download()\n\ndeployer.register()\n\n# Deploy the model to model serving using the following endpoint name\nendpoint_name = \"my-endpoint-name\"\n\ndeployer.deploy(endpoint_name)\n</code></pre>"},{"location":"getting-started/ezdeploy/#3-monitor-the-deployment","title":"3. Monitor the deployment","text":"<p>You will receive an url for the model serving endpoint. Monitor that url to see the status of the deployment.</p>"},{"location":"getting-started/ezdeploy/#querying-using-openai-sdk","title":"Querying using OpenAI SDK","text":"<p>The models are deployed as a pyfunc and they do not support natural json and need to fit the pyfunc spec. To allow you to use OpenAI, langchain, etc. we offer a compatability interface for those clients.</p> <pre><code>from mlflow_extensions.serving.compat.openai import OpenAI\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\n\nworkspace_host = spark.conf.get(\"spark.databricks.workspaceUrl\")\nendpoint_name = f\"https://{workspace_host}/serving-endpoints/{endpoint_name}/invocations\"\ntoken = get_databricks_host_creds().token\n\n# this is imported from mlflow_extensions.serving.compat.openai\nclient = OpenAI(\n    base_url=endpoint_name,\n    api_key=token\n)\n\nresponse = client.chat.completions.create(\n    # models will have their own name and will also have an alias called \"default\"\n    model=\"default\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Hi how are you?\"\n        }\n    ],\n)\n</code></pre>"},{"location":"getting-started/ezdeploy/#querying-using-langchain-sdk","title":"Querying using Langchain SDK","text":"<p>You can also use query the data using ChatOpenAI using langchain sdk.</p> <pre><code>from mlflow_extensions.serving.compat.langchain import ChatOpenAI\n\n# if you want to use completions\n# from mlflow_extensions.serving.compat.langchain import OpenAI\n\n# this ChatOpenAI is imported from mlflow_extensions.serving.compat.langchain\nmodel = ChatOpenAI(\n    model=\"default\",  # default is the alias for the model\n    base_url=\"https://&lt;&gt;.com/serving-endpoints/&lt;model-name&gt;\",\n    api_key=\"&lt;dapi...&gt;\"\n)\nmodel.invoke(\"hello world\")\n</code></pre>"},{"location":"getting-started/ezdeploylite/","title":"Deploying a model with EzDeployLite","text":""},{"location":"getting-started/ezdeploylite/#requirements","title":"Requirements","text":"<ol> <li>You need access to GPUs (T4, A10, A100, or H100) in your cloud account</li> <li>Your region needs to support gpus (T4, A10, A100, or H100)</li> <li>You need to have access to any compute to run the script from a notebook. (Serverless or interactive)</li> </ol>"},{"location":"getting-started/ezdeploylite/#what-is-ezdeploylite","title":"What is EzDeployLite?","text":"<p>EzDeployLite will take a prebuilt configuration and deploy it to job clusters and expose via driver proxy api. This is meant for dev and testing use cases. It will support either vLLM or SGLang as engines.</p>"},{"location":"getting-started/ezdeploylite/#deployment-steps","title":"Deployment Steps","text":""},{"location":"getting-started/ezdeploylite/#1-install-the-library","title":"1. Install the library","text":"<pre><code>%pip install mlflow-extensions\ndbutils.library.restartPython()\n</code></pre>"},{"location":"getting-started/ezdeploylite/#2-identify-the-model-to-deploy","title":"2. Identify the model to deploy","text":"<p>In this scenario</p> <pre><code>from mlflow_extensions.databricks.deploy.ez_deploy import EzDeployLite\nfrom mlflow_extensions.databricks.prebuilt import prebuilt\n\ndeployer = EzDeployLite(\n  ez_deploy_config=prebuilt.vision.vllm.QWEN2_VL_7B_INSTRUCT\n)\n\ndeployment_name = \"my_qwen_model\"\n# this will return a job run url where the model is deployed and running\ndeployer.deploy(deployment_name)\n</code></pre> <p>NOTE: THE MODEL WILL RUN INDEFINITELY AND NOT SCALE TO ZERO</p>"},{"location":"getting-started/ezdeploylite/#3-monitor-the-deployment","title":"3. Monitor the deployment","text":"<p>You will receive an url for the job run url. Monitor that url to see the status of the deployment.</p>"},{"location":"getting-started/ezdeploylite/#querying-using-openai-sdk","title":"Querying using OpenAI SDK","text":"<p>The models are deployed as a pyfunc and they do not support natural json and need to fit the pyfunc spec. To allow you to use OpenAI, langchain, etc. we offer a compatability interface for those clients.</p> <p>Make sure you install the latest version of openai sdk.</p> <pre><code>%pip install -U openai\ndbutils.library.restartPython()\n</code></pre> <pre><code>from openai import OpenAI\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\nfrom mlflow_extensions.serving.compat import get_ezdeploy_lite_openai_url\n\ndeployment_name = \"my_qwen_model\"\nbase_url = get_ezdeploy_lite_openai_url(deployment_name)\n\nclient = OpenAI(base_url=base_url, api_key=get_databricks_host_creds().token)\nfor i in client.models.list():\n    model = i.id\n\nresponse = client.chat.completions.create(\n    # models will have their own name and will also have an alias called \"default\"\n    model=\"default\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Hi how are you?\"\n        }\n    ],\n)\n</code></pre>"},{"location":"getting-started/ezdeploylite/#querying-using-langchain-sdk","title":"Querying using Langchain SDK","text":"<p>You can also use query the data using ChatOpenAI using langchain sdk.</p> <pre><code>from mlflow_extensions.serving.compat.langchain import ChatOpenAI\n# if you want to use completions\n# from mlflow_extensions.serving.compat.langchain import OpenAI\n\n# this ChatOpenAI is imported from mlflow_extensions.serving.compat.langchain\nfrom mlflow_extensions.serving.compat import get_ezdeploy_lite_openai_url\n\ndeployment_name = \"my_qwen_model\"\nbase_url = get_ezdeploy_lite_openai_url(deployment_name)\n\nmodel = ChatOpenAI(\n    model=\"default\", # default is the alias for the model\n    base_url=base_url, \n    api_key=\"&lt;dapi...&gt;\"\n)\nmodel.invoke(\"what color is the sky?\")\n</code></pre>"},{"location":"getting-started/ezdeploylite/#registering-into-mosaic-ai-gateway","title":"Registering into Mosaic AI Gateway","text":""},{"location":"getting-started/ezdeploylite/#requirements_1","title":"Requirements","text":"<p>To register into Mosaic AI Gateway you need the following things:</p> <ol> <li>Base URL of the deployment</li> <li>Token to the workspace</li> <li>Model deployment name (this will always be <code>default</code> for vllm models)</li> </ol> <p>To retrieve the base_url you can run this on the workspace where the model is deployed:</p> <pre><code>from mlflow_extensions.serving.compat import get_ezdeploy_lite_openai_url\n\ndeployment_name = \"my_qwen_model\"\nbase_url = get_ezdeploy_lite_openai_url(deployment_name)\n</code></pre> <p>To retrieve the token its basically the databricks token of the user who deployed the model.</p> <p>The following steps will show you what it looks like in the Databricks UI</p>"},{"location":"getting-started/ezdeploylite/#setting-up-a-new-mosaic-ai-gateway-endpoint","title":"Setting up a new Mosaic AI Gateway Endpoint","text":""},{"location":"getting-started/ezdeploylite/#setting-up-the-external-openai-endpoint","title":"Setting up the external OpenAI endpoint","text":""},{"location":"getting-started/ezdeploylite/#configure-the-settings","title":"Configure the settings","text":"<ol> <li>Make sure you set the OpenAI API Base (look at the requirements for the base url)</li> <li>Make sure you set the external model name to default (you can just type in the input)</li> <li>Ensure that you set the OpenAI API key secret to the databricks token</li> </ol>"},{"location":"getting-started/guided-decoding/","title":"Guided Decoding","text":"<p>The goal of guided decoding is during the decode step of the generation we can control available tokens by applying a bias to the output. You can learn more from the outlines paper here https://arxiv.org/abs/2307.09702.</p> <p>The key here is this is done during generation and provides some degree of gaurantees. This is how function calling works.</p> <p>If you want to learn more about this in detail you can read this: https://lmsys.org/blog/2024-02-05-compressed-fsm/ by the SGLang team.</p>"},{"location":"getting-started/guided-decoding/#enabling-guided-decoding","title":"Enabling Guided Decoding","text":"<p>The models that support guided decoding automatically have this feature enabled using outlines as the backend.</p>"},{"location":"getting-started/guided-decoding/#using-guided-decoding-with-pydantic","title":"Using Guided Decoding with Pydantic","text":"<p>Pydantic is a great way to validate the input and output of the model. You can use python, classes and fields to constrain the output of the model into a fixed json schema.</p> <pre><code>from pydantic import BaseModel\n\nclass Data(BaseModel):\n  outside: bool\n  inside: bool\n\n#  construct your client using the guide in ezdeploy or ezdeploylite\nclient = OpenAI(base_url=..., api_key=...)\nresponse = client.chat.completions.create(\n  model=\"default\",\n  messages=[\n    {\"role\": \"user\", \"content\": [\n                {\"type\": \"text\", \"text\": \"Is the image indoors or outdoors?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                      \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n                    },\n                },\n            ],\n     }\n  ],\n  #   if you want to use guided decoding to improve performance and control output\n  extra_body={\n    \"guided_json\": Data.schema()\n  }\n)\n</code></pre>"},{"location":"getting-started/guided-decoding/#guided-decoding-options","title":"Guided Decoding options","text":"<p>There are a few options that you can use to control the output of the model.</p> <ol> <li>guided_json: This is a pydantic schema that you can use to control the output of the model. This is a json    schema.</li> <li>guided_regex: This is a regex that you can use to control the output of the model. This is a string.</li> <li>guided_choice: This is a list of choices that you can use to control the output of the model. This is a list of    strings.</li> <li>guided_grammar: This is context free grammar that you can use to control the output of the model. This is a    string.</li> </ol>"},{"location":"getting-started/setup/","title":"Setup","text":"<p>MLflow extensions deployments are split into two main parts, <code>EzDeployLite</code> and <code>EzDeploy</code>. <code>EzDeployLite</code> is a lightweight deployment that is meant for development and testing. It runs on jobs service and uses driver proxy. <code>EzDeploy</code> is a full deployment that is meant for production which gets deployed into model serving. EzDeployLite deploys to AWS, Azure and GCP. EzDeploy deploys to any regions supporting model serving with gpus.</p>"},{"location":"getting-started/setup/#1-requirements","title":"1: Requirements","text":"<ol> <li>You need access to create a job or a cluster with gpus T4, A10, A100 or H100.</li> <li>You need access to a cluster to run the deployment (any compute, serverless, or non serverless)</li> <li>Ability to download a model from huggingface or any other source.</li> <li>Ability to install mlflow-extensions.</li> </ol>"},{"location":"getting-started/setup/#2-installation","title":"2: Installation","text":"<pre><code>%pip install mlflow-extensions\ndbutils.library.restartPython()\n</code></pre> <p>The previous command installs the mlflow-extensions library. You can then import the library and use it in your code. Run through the EzDeployLite or EzDeploy examples to see how to use the library for deploying models.</p>"},{"location":"getting-started/sglang-runtime/","title":"SGLang SDK [WIP]","text":""},{"location":"getting-started/tool-calling/","title":"Tool Calling [TBD]","text":""},{"location":"getting-started/vision-models/","title":"Vision Models","text":"<p>Vision models or VLMs are models that are trained on vision tasks. These models can be used for various vision tasks like image classification, object detection, image segmentation, etc. You can provide things like charts, images, etc. to these models and they will provide you with the desired output.</p> <p>You can use <code>EzDeploy</code> or <code>EzDeployLite</code> to deploy these models to model serving. Read the previous guides. For the scope of this we will be using <code>EzDeployLite</code> to deploy a vision model.</p>"},{"location":"getting-started/vision-models/#ezdeploylite","title":"EzDeployLite","text":"<pre><code>%pip install mlflow-extensions==0.14.0\ndbutils.library.restartPython()\n\nfrom mlflow_extensions.databricks.deploy.ez_deploy import EzDeployLite\nfrom mlflow_extensions.databricks.prebuilt import prebuilt\n\ndeployer = EzDeployLite(\n    ez_deploy_config=prebuilt.vision.vllm.QWEN2_VL_7B_INSTRUCT\n)\n\ndeployment_name = \"my_qwen_model\"\ndeployer.deploy(deployment_name)\n</code></pre> <p>The code will deploy a vision model to Databricks jobs and expose the model as via a proxy. This is meant for dev and testing use cases.</p>"},{"location":"getting-started/vision-models/#querying-using-openai-sdk-for-vision-models","title":"Querying using OpenAI SDK for Vision Models","text":""},{"location":"getting-started/vision-models/#using-image-urls","title":"Using Image URLs","text":"<p>You can query the model using image URLs. The model will return the desired output based on the image provided.</p> <p>Installation:</p> <pre><code>%pip install mlflow-extensions==0.14.0\n%pip install -U openai\ndbutils.library.restartPython()\n</code></pre> <p>Query Model:</p> <pre><code>from mlflow_extensions.serving.compat import get_ezdeploy_lite_openai_url\n\ndeployment_name = \"my_qwen_model\"\nbase_url = get_ezdeploy_lite_openai_url(deployment_name)\n\nfrom openai import OpenAI\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\n\nclient = OpenAI(base_url=base_url, api_key=get_databricks_host_creds().token)\nfor i in client.models.list():\n    model = i.id\n\nprint(model)\n\nresponse = client.chat.completions.create(\n    model=\"default\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Is the image indoors or outdoors?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n                    },\n                },\n            ],\n        }\n    ],\n)\nprint(response)\n</code></pre>"},{"location":"getting-started/vision-models/#using-images-from-base64-encoded-strings","title":"Using images from base64 encoded strings","text":"<p>You can also query the model using images from the file system. The model will return the desired output based on the image provided.</p> <p>Installation:</p> <pre><code>%pip install mlflow-extensions==0.14.0\n%pip install -U openai\ndbutils.library.restartPython()\n</code></pre> <p>Download Image into Base64; you can modify this to fetch from file system but the logic would be the same.</p> <p>Query Model:</p> <pre><code>import base64\nimport requests\nfrom mlflow_extensions.serving.compat import get_ezdeploy_lite_openai_url\n\ndeployment_name = \"my_qwen_model\"\nbase_url = get_ezdeploy_lite_openai_url(deployment_name)\n\nfrom openai import OpenAI\nfrom mlflow.utils.databricks_utils import get_databricks_host_creds\n\nclient = OpenAI(base_url=base_url, api_key=get_databricks_host_creds().token)\nfor i in client.models.list():\n    model = i.id\n\nprint(model)\n\n\n## Use base64 encoded image in the payload\ndef encode_image_base64_from_url(image_url: str) -&gt; str:\n    \"\"\"Encode an image retrieved from a remote url to base64 format.\"\"\"\n\n    # you can modify this with reading an image from a file\n    with requests.get(image_url) as response:\n        response.raise_for_status()\n        result = base64.b64encode(response.content).decode('utf-8')\n\n    return result\n\n\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\nimage_base64 = encode_image_base64_from_url(image_url=image_url)\nchat_completion_from_base64 = client.chat.completions.create(\n    model=\"default\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"What\u2019s in this image?\"\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n                },\n            },\n        ],\n    }],\n    max_tokens=256,\n)\nprint(chat_completion_from_base64)\n</code></pre>"},{"location":"supported-models/audio-models/","title":"Audio Models","text":"Config Huggingface Link context_length min_azure_ep_type_gpu min_aws_ep_type_gpu prebuilt.audio.vllm.FIXIE_ULTRA_VOX_0_4_64K_CONFIG https://huggingface.co/fixie-ai/ultravox-v0_4 64000 GPU_LARGE [A100_80Gx1 80GB] MULTIGPU_MEDIUM [A10Gx4 96GB] prebuilt.audio.vllm.FIXIE_ULTRA_VOX_0_4_128K_CONFIG https://huggingface.co/fixie-ai/ultravox-v0_4 Default GPU_LARGE_2 [A100_80Gx2 160GB] GPU_MEDIUM_8 [A10Gx8 192GB]"},{"location":"supported-models/embedding-models/","title":"Embedding Models [TBD]","text":""},{"location":"supported-models/text-models/","title":"Text Models","text":"Config Huggingface Link context_length min_azure_ep_type_gpu min_aws_ep_type_gpu prebuilt.text.sglang.GEMMA_2_9B_IT https://huggingface.co/google/gemma-2-9b-it Default GPU_LARGE [A100_80Gx1 80GB] MULTIGPU_MEDIUM [A10Gx4 96GB] prebuilt.text.sglang.META_LLAMA_3_1_8B_INSTRUCT_CONFIG https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct Default GPU_LARGE [A100_80Gx1 80GB] MULTIGPU_MEDIUM [A10Gx4 96GB] prebuilt.text.vllm.NUEXTRACT https://huggingface.co/numind/NuExtract Default GPU_LARGE [A100_80Gx1 80GB] GPU_MEDIUM [A10Gx1 24GB] prebuilt.text.vllm.NUEXTRACT_TINY https://huggingface.co/numind/NuExtract-tiny Default GPU_SMALL [T4x1 16GB] GPU_SMALL [T4x1 16GB] prebuilt.text.vllm.NOUS_HERMES_3_LLAMA_3_1_8B_64K https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B 64000 GPU_LARGE [A100_80Gx1 80GB] MULTIGPU_MEDIUM [A10Gx4 96GB] prebuilt.text.vllm.NOUS_HERMES_3_LLAMA_3_1_8B_128K https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B Default GPU_LARGE_2 [A100_80Gx2 160GB] GPU_MEDIUM_8 [A10Gx8 192GB] prebuilt.text.vllm.COHERE_FOR_AYA_23_35B https://huggingface.co/CohereForAI/aya-23-35B Default GPU_LARGE [A100_80Gx1 80GB] MULTIGPU_MEDIUM [A10Gx4 96GB] prebuilt.text.vllm.QWEN2_5_7B_INSTRUCT https://huggingface.co/Qwen/Qwen2.5-7B-Instruct Default GPU_LARGE [A100_80Gx1 80GB] MULTIGPU_MEDIUM [A10Gx4 96GB] prebuilt.text.vllm.QWEN2_5_14B_INSTRUCT https://huggingface.co/Qwen/Qwen2.5-14B-Instruct Default GPU_LARGE [A100_80Gx1 80GB] MULTIGPU_MEDIUM [A10Gx4 96GB] prebuilt.text.vllm.QWEN2_5_32B_INSTRUCT https://huggingface.co/Qwen/Qwen2.5-32B-Instruct Default GPU_LARGE_2 [A100_80Gx2 160GB] MULTIGPU_MEDIUM [A10Gx4 96GB] prebuilt.text.vllm.QWEN2_5_72B_8K_INSTRUCT https://huggingface.co/Qwen/Qwen2.5-72B-Instruct 8192 GPU_LARGE_2 [A100_80Gx2 160GB] GPU_MEDIUM_8 [A10Gx8 192GB] prebuilt.text.vllm.QWEN2_5_72B_INSTRUCT https://huggingface.co/Qwen/Qwen2.5-72B-Instruct Default GPU_LARGE_8 [A100_80Gx8 640GB] GPU_LARGE_8 [A100_80Gx8 640GB]"},{"location":"supported-models/vision-models/","title":"Vision Models","text":"Config Huggingface Link context_length min_azure_ep_type_gpu min_aws_ep_type_gpu prebuilt.vision.sglang.LLAVA_NEXT_LLAMA3_8B https://huggingface.co/lmms-lab/llama3-llava-next-8b Default GPU_LARGE [A100_80Gx1 80GB] MULTIGPU_MEDIUM [A10Gx4 96GB] prebuilt.vision.sglang.LLAVA_NEXT_QWEN_1_5_72B_CONFIG https://huggingface.co/lmms-lab/llama3-llava-next-8b Default GPU_LARGE_2 [A100_80Gx2 160GB] GPU_MEDIUM_8 [A10Gx8 192GB] prebuilt.vision.sglang.LLAVA_ONEVISION_QWEN_2_7B_CONFIG https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov Default GPU_LARGE [A100_80Gx1 80GB] MULTIGPU_MEDIUM [A10Gx4 96GB] prebuilt.vision.sglang.LLAVA_ONEVISION_QWEN_2_72B_CONFIG https://huggingface.co/lmms-lab/llava-onevision-qwen2-72b-ov Default GPU_LARGE_2 [A100_80Gx2 160GB] GPU_MEDIUM_8 [A10Gx8 192GB] prebuilt.vision.vllm.PHI_3_5_VISION_INSTRUCT_4K https://huggingface.co/microsoft/Phi-3.5-vision-instruct 4096 GPU_SMALL [T4x1 16GB] GPU_SMALL [T4x1 16GB] prebuilt.vision.vllm.PHI_3_5_VISION_INSTRUCT_8K https://huggingface.co/microsoft/Phi-3.5-vision-instruct 8192 GPU_SMALL [T4x1 16GB] GPU_SMALL [T4x1 16GB] prebuilt.vision.vllm.PHI_3_5_VISION_INSTRUCT_12K https://huggingface.co/microsoft/Phi-3.5-vision-instruct 12000 GPU_LARGE [A100_80Gx1 80GB] GPU_MEDIUM [A10Gx1 24GB] prebuilt.vision.vllm.PHI_3_5_VISION_INSTRUCT_32K https://huggingface.co/microsoft/Phi-3.5-vision-instruct 32000 GPU_LARGE [A100_80Gx1 80GB] MULTIGPU_MEDIUM [A10Gx4 96GB] prebuilt.vision.vllm.PHI_3_5_VISION_INSTRUCT_64K https://huggingface.co/microsoft/Phi-3.5-vision-instruct 64000 GPU_LARGE [A100_80Gx1 80GB] MULTIGPU_MEDIUM [A10Gx4 96GB] prebuilt.vision.vllm.PHI_3_5_VISION_INSTRUCT_128K https://huggingface.co/microsoft/Phi-3.5-vision-instruct Default GPU_LARGE_2 [A100_80Gx2 160GB] GPU_MEDIUM_8 [A10Gx8 192GB] prebuilt.vision.vllm.QWEN2_VL_2B_INSTRUCT https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct Default GPU_LARGE [A100_80Gx1 80GB] GPU_MEDIUM [A10Gx1 24GB] prebuilt.vision.vllm.QWEN2_VL_7B_INSTRUCT https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct Default GPU_LARGE [A100_80Gx1 80GB] MULTIGPU_MEDIUM [A10Gx4 96GB] prebuilt.vision.vllm.PIXTRAL_12B_32K_INSTRUCT https://huggingface.co/mistralai/Pixtral-12B-2409 32768 GPU_LARGE [A100_80Gx1 80GB] MULTIGPU_MEDIUM [A10Gx4 96GB] prebuilt.vision.vllm.PIXTRAL_12B_64K_INSTRUCT https://huggingface.co/mistralai/Pixtral-12B-2409 65536 GPU_LARGE [A100_80Gx1 80GB] MULTIGPU_MEDIUM [A10Gx4 96GB] prebuilt.vision.vllm.PIXTRAL_12B_128K_INSTRUCT https://huggingface.co/mistralai/Pixtral-12B-2409 Default GPU_LARGE_2 [A100_80Gx2 160GB] GPU_MEDIUM_8 [A10Gx8 192GB]"}]}